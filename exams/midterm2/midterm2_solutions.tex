\documentclass[12pt]{article}

\include{preamble}

\newcommand{\instr}{\small Your answer will consist of a lowercase string (e.g. \texttt{aebgd}) where the order of the letters does not matter. \normalsize}

\title{Math 340 / 640 Fall \the\year{} \\ Midterm Examination Two \inred{Solutions}}
\author{Professor Adam Kapelner}

\date{November 6, \the\year{}}

\begin{document}
\maketitle

\noindent Full Name \line(1,0){410}

\thispagestyle{empty}

\section*{Code of Academic Integrity}

\footnotesize
Since the college is an academic community, its fundamental purpose is the pursuit of knowledge. Essential to the success of this educational mission is a commitment to the principles of academic integrity. Every member of the college community is responsible for upholding the highest standards of honesty at all times. Students, as members of the community, are also responsible for adhering to the principles and spirit of the following Code of Academic Integrity.

Activities that have the effect or intention of interfering with education, pursuit of knowledge, or fair evaluation of a student's performance are prohibited. Examples of such activities include but are not limited to the following definitions:

\paragraph{Cheating} Using or attempting to use unauthorized assistance, material, or study aids in examinations or other academic work or preventing, or attempting to prevent, another from using authorized assistance, material, or study aids. Example: using an unauthorized cheat sheet in a quiz or exam, altering a graded exam and resubmitting it for a better grade, etc.\\
\\
\noindent I acknowledge and agree to uphold this Code of Academic Integrity. \\~\\

\begin{center}
\line(1,0){350} ~~~ \line(1,0){100}\\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~signature~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ date
\end{center}

\normalsize

\section*{Instructions}
This exam is 110 minutes (variable time per question) and closed-book. You are allowed \textbf{two} 8.5'' $\times$ 11'' page (front and back) \qu{cheat sheets}, blank scrap paper (provided by the proctor) and a graphing calculator (which is not your smartphone). Please read the questions carefully. Within each problem, I recommend considering the questions that are easy first and then circling back to evaluate the harder ones. Show as much partial work as you can and justify each step. No food is allowed, only drinks. %If the question reads \qu{compute,} this means the solution will be a number otherwise you can leave the answer in \textit{any} widely accepted mathematical notation which could be resolved to an exact or approximate number with the use of a computer. I advise you to skip problems marked \qu{[Extra Credit]} until you have finished the other questions on the exam, then loop back and plug in all the holes. I also advise you to use pencil. The exam is 100 points total plus extra credit. Partial credit will be granted for incomplete answers on most of the questions. \fbox{Box} in your final answers. Good luck!

\pagebreak


\problem Below are mostly unrelated problems.

\begin{enumerate}[(a)]

\subquestionwithpoints{6} Let $\mathcal{E} \sim \stdnormnot$. Assume $\expe{\mathcal{E}} = 0$ without proof. Show that $\mathcal{E}$ qualifies as an \qu{error distribution}.\\

\inred{

The following is the density of the normal: $\mathcal{E} \sim  f_{\mathcal{E}}(\epsilon) = \stdnormnot = \oneoversqrt{2\pi}e^{-\epsilon^2 / 2}$. Error distributions are defined by three conditions and these three are satisfied:

\begin{enumerate}[(I)]
\item Mean center i.e. $\expe{\mathcal{E}} = 0$. This is true as given.

\item Density is symmetric around zero i.e. $f_{\mathcal{E}}(\epsilon) = f_{\mathcal{E}}(-\epsilon)$. This is true as $\oneoversqrt{2\pi}e^{-\epsilon^2 / 2} = \oneoversqrt{2\pi}e^{-(-\epsilon)^2 / 2}$.

\item Density is strictly monotonically decreases in both directions i.e. $f_{\mathcal{E}}'(\epsilon) < 0$ for $\epsilon > 0$ and $f_{\mathcal{E}}'(\epsilon) > 0$ for $\epsilon < 0$. The derivative of the density is $f_{\mathcal{E}}'(\epsilon) = (-\epsilon)\oneoversqrt{2\pi} e^{-\epsilon^2 / 2} = -\epsilon f_{\mathcal{E}}(\epsilon)$ which is negative for $\epsilon > 0$ and positive for $\epsilon < 0$.
\end{enumerate}
}

\subquestionwithpoints{10} Given the jdf of dependent rv's $X_1$ and $X_2$, $f_{X_1, X_2}(x_1, x_2)$, find an integral formula for the PDF of $M = X_1 X_2$.

\inred{

\fbox{Answer \#1} Let $U = X_2$ thus $X_2 = h_2(M, U) = U$. Then we know $M = X_1 U$ thus $X_1 = h_1(M, U) = M / U$. We then have 

\beqn
\frac{\partial h_1}{\partial M} = \oneover{U}, ~~\frac{\partial h_1}{\partial U} = -M / U^2, ~~\frac{\partial h_2}{\partial M} = 0, ~~\frac{\partial h_2}{\partial U} = 1, ~~ |J_h| = |(1/U)(1) - (-M/U^2)(0)| = 1/|U|
\eeqn 

Putting it all together, $f_M(m) = \int_\reals f_{X_1, X_2}(h_1, h_2)|J_h| du = \int_\reals f_{X_1, X_2}\parens{\frac{m}{u}, u} \oneover{|u|} \,du$.


\fbox{Answer \#2} Let $U = X_1$ thus $X_1 = h_1(M, U) = U$. Then we know $M = U X_2$ thus $X_2 = h_2(M, U) = M / U$. We then have 

\beqn
\frac{\partial h_1}{\partial M} = 0, ~~\frac{\partial h_1}{\partial U} = 1, ~~\frac{\partial h_2}{\partial M} = 1/U, ~~\frac{\partial h_2}{\partial U} = -M / U^2, ~~ |J_h| = |(0)(-M / U^2) - (1)(1/U)| = 1/|U|
\eeqn 

Putting it all together, $f_M(m) = \int_\reals f_{X_1, X_2}(h_1, h_2)|J_h| du = \int_\reals f_{X_1, X_2}\parens{u, \frac{m}{u}} \oneover{|u|} \,du$.


}


\subquestionwithpoints{7} Let $X \sim \gammanot{\alpha}{\beta} := \frac{\beta^\alpha}{\Gamma\parens{\alpha}} x^{\alpha - 1} e^{-\beta x} \indic{x \in (0, \infty)}$. Prove $\expe{X} = \frac{\alpha}{\beta}$ without using ch.f's.

\inred{
\beqn
\hspace{-50px}\expe{X} = \int_\reals x \frac{\beta^\alpha}{\Gamma\parens{\alpha}} x^{\alpha - 1} e^{-\beta x} \indic{x \in (0, \infty)} dx = \frac{\beta^\alpha}{\Gamma\parens{\alpha}} \int_0^\infty x^{(\alpha + 1) - 1} e^{-\beta x} dx = \frac{\beta^\alpha}{\Gamma\parens{\alpha}} \frac{\Gamma(\alpha+1)}{\beta^{\alpha+1}} = \frac{\beta^\alpha}{\Gamma\parens{\alpha}} \frac{\alpha\Gamma(\alpha)}{\beta\beta^{\alpha}} =\frac{\alpha}{\beta}
\eeqn

where the third equality follows from the gamma-like integral $\displaystyle\int_\reals u^{k-1} e^{-cu}du = \dfrac{\Gamma(k)}{c^k}$.
}
\subquestionwithpoints{5} Let $X \sim \gammanot{17}{37}$. Find an upper bound on the probability that $\prob{X > 17}$. Hint: use the previous problem's result.\\

\inred{
Since $X$ is non-negative and by (c), $\expe{X} = \frac{\alpha}{\beta} = \frac{17}{37}$, we can employ Markov's inequality:

\beqn
\prob{X > a} \leq \frac{\expe{X}}{a} ~\Rightarrow~ \prob{X > 17} \leq \frac{\frac{17}{37}}{17} ~\Rightarrow~ \prob{X > 17} \leq \frac{1}{37}
\eeqn

}

\subquestionwithpoints{6} Let $X \sim \chisq{k}$. Prove that $\expe{X} = k$. \\

\inred{
Remember that $X \sim \chisq{k} = \gammanot{\frac{k}{2}}{\half}$ and by (c), $\expe{X} = \frac{\alpha}{\beta} = \frac{k/2}{1/2} = k$.\\
}
\subquestionwithpoints{6} Let $X \sim \gammanot{\alpha}{\beta} := \frac{\beta^\alpha}{\Gamma\parens{\alpha}} x^{\alpha - 1} e^{-\beta x} \indic{x \in (0, \infty)}$. Let $Y = \oneover{X}$. Find $f_Y(y)$. Simplify as much as you can. This is called the inverse gamma distribution.\\

\inred{
$Y = g(X) = \oneover{X}$ implies $X = g^{-1}(Y) = \frac{1}{Y}$ and its absolute derivative is $\abss{\frac{d}{dy}[g^{-1}(y)]} = y^{-2}$. Putting this all together we have:

\beqn
f_Y{y} = f_X(g^{-1}(y)) \abss{\frac{d}{dy}[g^{-1}(y)]} &=& \frac{\beta^\alpha}{\Gamma\parens{\alpha}} \parens{\oneover{y}}^{\alpha - 1} e^{-\beta \parens{\oneover{y}}} \indic{\parens{\oneover{y}} \in (0, \infty)} y^{-2} \\
&=& \frac{\beta^\alpha}{\Gamma\parens{\alpha}} y^{-\alpha + 1} y^{-2} e^{-\frac{\beta}{y}} \indic{y \in (0, \infty)}  \\
&=& \frac{\beta^\alpha}{\Gamma\parens{\alpha}} y^{-\alpha - 1} e^{-\frac{\beta}{y}} \indic{y \in (0, \infty)} \\
\eeqn

}

\subquestionwithpoints{6} Let $X \sim T_{k}$. Let $Y = \mu + \sigma X$ where $\mu \in \reals$ and $\sigma > 0$. Find $f_Y(y)$.\\

\inred{
\beqn
X \sim  f_X(x)  &=& \frac{\Gamma\parens{\overtwo{k+1}}}{\sqrt{\pi k } \Gamma\parens{\overtwo{k}}} \tothepow{1 + \frac{x^2}{k}}{-\overtwo{k+1}} \\
Y \sim f_Y(y) &=& \oneover{|\sigma|} f_X\parens{\frac{y-\mu}{\sigma}} \\
&=& \frac{\Gamma\parens{\overtwo{k+1}}}{\sqrt{\pi k \sigsq} \Gamma\parens{\overtwo{k}}} \tothepow{1 + \frac{(y-\mu)^2}{k\sigsq}}{-\overtwo{k+1}}
\eeqn
}\pagebreak

\subquestionwithpoints{5} If $\X \sim \text{Multinom}\parens{17, \bracks{\frac{1}{2} ~\frac{1}{3} ~\frac{1}{6}}^\top}$, compute $\cov{X_1}{X_3}$.\\

\inred{
\beqn
\cov{X_1}{X_3} = -n p_1 p_3 = -17 \cdot \frac{1}{2} \cdot \frac{1}{6} = -\frac{17}{12}
\eeqn
}

\subquestionwithpoints{6} Let $X \sim \binomial{n}{p}$. Let $Y = \natlog{X+1}$. Find $p_Y(y)$. \\

\inred{
$Y = g(X) = \natlog{X+1}$ and thus $X = g^{-1}(Y) = e^Y - 1$ and thus 

\beqn
p_Y(y) = p_X(g^{-1}(y)) = p_X(e^y - 1) = \binom{n}{e^y - 1} p^{e^y - 1} (1-p)^{n - e^y + 1}
\eeqn
}


\subquestionwithpoints{10} Prove the following and justify each step with theorems and results from class:

\beqn
\frac{\Xbar - \mu}{\frac{S_n}{\sqrt{n}}} \convd \stdnormnot
\eeqn 

\inred{
In class, we proved $S^2_n \convp \sigsq$ and thus by the CMT we have $S_n \convp \sigma$ where $g(x) = \sqrt{x}$. Again by the CMT we have 

\beqn
S_n \convp \sigma ~\Rightarrow~ \frac{\frac{\sigma}{\sqrt{n}}}{\frac{S_n}{\sqrt{n}}} \convp 1~~\text{where} ~~g(x) = \frac{\frac{\sigma}{\sqrt{n}}}{\frac{x}{\sqrt{n}}}
\eeqn

We also have the CLT result from class:



\beqn
\frac{\Xbar - \mu}{\frac{\sigma}{\sqrt{n}}} \convd \stdnormnot
\eeqn

Via the clever algebra trick from class,

\beqn
\frac{\Xbar - \mu}{\frac{S_n}{\sqrt{n}}} = \underbrace{\frac{\frac{\sigma}{\sqrt{n}}}{\frac{S_n}{\sqrt{n}}}}_{A_n} \underbrace{\frac{\Xbar - \mu}{\frac{\sigma}{\sqrt{n}}}}_{B_n} \convd \stdnormnot
\eeqn

where the $\convd \stdnormnot$ above follows from Slutsky's theorem $\textcircled{\tiny{A}}$ as $A_n \convp 1$ and $B_n \convd \stdnormnot$.

}\pagebreak

\subquestionwithpoints{8} Let $X_n \sim \text{Weibull}(k, n)$ where $k > 0$. Prove $X_n \convp 0$ from the definition of convergence in probability.\\

\inred{
The $X_n \sim \text{Weibull}(k, n)$ rv is non-negative with CDF $F_{X_n}(x) =  1 - e^{-(nx)^k}$ and thus $\prob{X_n > \epsilon} = 1 - F_{X_n}(x) =  e^{-(nx)^k}$. By the definition, $X_n \convp 0$ means:

\beqn
\forall \epsilon > 0,~\limitn \prob{|X_n - 0| > \epsilon} = 0
\eeqn

We pick any positive $\epsilon$ and compute:

\beqn
\limitn \prob{|X_n - 0| > \epsilon} = \limitn \prob{X_n > \epsilon} = \limitn e^{-(n\epsilon)^k} = \lim_{u \rightarrow \infty}  \oneover{e^{c u}} = 0
\eeqn

where $u = n^k$, $c = \epsilon^k > 0$ and the first equality above follows from the non-negativity of $X$ allowing us to drop the absolute value within the probability operator.

}

\subquestionwithpoints{7} If $X \sim \text{ParetoI}(k = 0.17, \lambda = 2.37)$, then $\expe{X} = 0.294$ and $\var{X} =  0.0986$. Let $X_1, X_2 \iid \text{ParetoI}(k = 0.17, \lambda = 2.37)$, find an upper bound on $\prob{X_1 + X_2 > 3.14}$ to the nearest three significant digits.\\

\inred{
\fbox{Preferred Answer} By Chebyshev's inequality corollary from class, if $b > 2\mu$ we have

\beqn
\prob{Y > b} \leq \frac{\sigsq_Y}{(b - \mu_Y)^2}
\eeqn

Since $b = 3.14 > 2\mu = 2\cdot0.294 = 0.588$ we can employ this inequality and then substitute our numeric values to find:

\beqn
\prob{X_1 + X_2 > 3.14} \leq \frac{\var{X_1 + X_2}}{(3.14 - \expe{X_1 + X_2})^2} = \frac{2\sigsq}{(3.14 - 2\mu)^2} = \frac{2 \cdot 0.0986}{(3.14 - 2 \cdot 0.294)^2} = 0.0303
\eeqn

\fbox{Acceptable Answer} By Markov's inequality and the fact that $X_1, X_2$ are both non-negative so their convolution is non-negative, we can employ this inequality and then substitute our numeric values to find:

\beqn
\prob{X_1 + X_2 > 3.14} \leq \frac{\expe{X_1 + X_2}}{3.14} = \frac{2\mu}{3.14}  = \frac{2 \cdot 0.294}{3.14} = 0.187
\eeqn
}


\subquestionwithpoints{8} Let $Z_1, Z_2 \iid \stdnormnot$. Let $X = Z_1 / Z_2$. What is the value of $\phi_X'(0)$?\\

\inred{
The rv $Z_1 / Z_2$ is Cauchy-distributed which means $\expe{X}$ is undefined. Since $\phi_X'(0) = \frac{\expe{X}}{i}$, the value of $\phi_X'(0)$ is undefined.
}\pagebreak

\subquestionwithpoints{10} Let $Z_1, Z_2 \iid \stdnormnot$. Let $X = \half Z_1^2 + Z_1 Z_2 + \half Z_2^2$. How is $X$ distributed?\\

\inred{

\fbox{Simplest way thanks to Bryan} We can write $X = \half(Z_1 - Z_2)^2$ Since $Z_1, Z_2 \iid \stdnormnot$, we know that $U := Z_1 - Z_2 \sim \normnot{0}{2}$. We can then write 

\beqn
X = \half U^2 = \parens{\fourth U}^2.
\eeqn

We know based on our transformation equations for the normal that $\fourth U \sim \stdnormnot$ which means $X \sim \chisq{1}$.


\fbox{Via Cochran's theorem} Let $\bv{Z} = \bracks{Z_1~Z_2}^\top$. We now want a quadratic form with vector $\Z$, so we need to guess a determining matrix which we call $B_1$ and show this is an equivalent expression for our target rv $X$:

\vspace{-0.3cm}
\beqn
X = \bv{Z}^\top B_1 \bv{Z} &=& \bracks{Z_1~Z_2} \underbrace{\twobytwomat{1/2}{1/2}{1/2}{1/2}}_{B_1} \twovec{Z_1}{Z_2} =  \bracks{Z_1~Z_2} \twovec{\half Z_1 + \half Z_2}{\half Z_1 + \half Z_2} \\
&=& \half Z_1^2 + \half Z_1 Z_2 + \half Z_2 Z_1+ \half Z_2^2 = \half Z_1^2 + Z_1 Z_2 + \half Z_2^2
\eeqn

This matrix $B_1$ has $\rank{B_1} = 1$. To use Cochran's theorem, we conjure a matrix $B_2$:

\beqn
B_2 = \twobytwomat{1/2}{-1/2}{-1/2}{1/2}
\eeqn

which has $\rank{B_2} = 1$ which can be seen by taking the negative of column 2 and noticing it is the same as column 1. 

Putting the facts above together, $B_1 + B_2 = I_2$ and  $\rank{B_1} + \rank{B_2} = 2$. We now satisfied the conditions for Cochran's theorem and one of the three equivalent results. Thus, Cochran's Thm's implies the additional result that the quadratic forms are chi-squared distributed. This result gives us our answer for the distribution of $X$:

\vspace{-0.3cm}
\beqn
X = \bv{Z}^\top B_1 \bv{Z} \sim \chisq{\rank{B_1}} = \chisq{1}
\eeqn

%\fbox{Via Variable Transformation} We can write $X = \half(Z_1 - Z_2)^2$ Since $Z_1, Z_2 \iid \stdnormnot$, we know that $Z_1 - Z_2 \sim \normnot{0}{2}$. Let $U = Z_1 - Z_2$. We now seek to derive the density of $Y$ where $Y = U^2$:
%
%\vspace{-0.3cm}
%\beqn
%F_Y(y) &:=& \prob{Y \leq y} = \prob{U^2 \leq y} = \prob{U \in \bracks{-\sqrt{y}, \sqrt{y}}} = 2\prob{U \in \bracks{0, \sqrt{y}}} \\
%&=& 2\parens{F_U(\sqrt{y}) - F_U(0)} = 2\parens{F_U(\sqrt{y}) - \half} \\
%f_Y(y) &=& \frac{d}{dy}\bracks{F_Y(y)} = 2 \frac{d}{dy}\bracks{F_U(\sqrt{y}} = 2y^{-1/2} f_U(\sqrt{y}) = 2\parens{\half y^{-1/2}} \oneoversqrt{4\pi} e^{-\frac{(\sqrt{y})^2}{4}} \indic{\sqrt{y} \in (-\infty, \infty)} \\
%&=& \oneoversqrt{4\pi} y^{-1/2} e^{-\frac{y}{4}} \indic{y \in (0, \infty)}
%\eeqn
%
%Since $X = \half Y$, we can use the linear scaling formula:
%
%\beqn
%f_X(x) = 2 f_Y(2x) = 2\oneoversqrt{4\pi}  (2x)^{-1/2} e^{-\frac{2x}{4}} \indic{2x \in (0, \infty)} = \oneoversqrt{2\pi} x^{-1/2} e^{-x/2} \indic{x \in (0, \infty)} = \chisq{1}
%\eeqn

\fbox{Via Multivariate Transformation} I do not think this is a possible route as the function $X = \half(Z_1 - Z_2)^2$ is not invertible with a dummy dimension.
}

%set.seed(1)
%Nsim = 1e5
%
%x = array(NA, Nsim)
%for (i in 1 : Nsim){
%  zs = rnorm(2)
%  x[i] = 0.5 * zs[1]^2 +  zs[1] * zs[2] + 0.5 * zs[2]^2
%}
%
%ks.test(x, "pchisq", 1) #Cochran was smart!
\end{enumerate}

\end{document}
